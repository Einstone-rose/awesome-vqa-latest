@inproceedings{lu202012,
  title={12-in-1: Multi-task vision and language representation learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10437--10446},
  year={2020}
}

@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={235--251},
  year={2016},
  organization={Springer}
}

@article{ilievski2016focused,
  title={A focused dynamic attention model for visual question answering},
  author={Ilievski, Ilija and Yan, Shuicheng and Feng, Jiashi},
  journal={arXiv preprint arXiv:1604.01485},
  year={2016}
}

@inproceedings{malinowski2014multi,
  title={A multi-world approach to question answering about real-world scenes based on uncertain input},
  author={Malinowski, Mateusz and Fritz, Mario},
  booktitle={Advances in neural information processing systems},
  pages={1682--1690},
  year={2014}
}

@article{shrestha2020negative,
  title={A negative case analysis of visual grounding methods for VQA},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  journal={arXiv preprint arXiv:2004.05704},
  year={2020}
}

@inproceedings{huang2019novel,
  title={A novel framework for robustness analysis of visual qa models},
  author={Huang, Jia-Hong and Dao, Cuong Duc and Alfadly, Modar and Ghanem, Bernard},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8449--8456},
  year={2019}
}

@inproceedings{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  booktitle={Advances in neural information processing systems},
  pages={4967--4976},
  year={2017}
}

@article{chen2015abc,
  title={Abc-cnn: An attention based convolutional neural network for visual question answering},
  author={Chen, Kan and Wang, Jiang and Chen, Liang-Chieh and Gao, Haoyuan and Xu, Wei and Nevatia, Ram},
  journal={arXiv preprint arXiv:1511.05960},
  year={2015}
}

@article{grand2019adversarial,
  title={Adversarial regularization for visual question answering: Strengths, shortcomings, and side effects},
  author={Grand, Gabriel and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:1906.08430},
  year={2019}
}

@inproceedings{kafle2017analysis,
  title={An analysis of visual question answering algorithms},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1965--1973},
  year={2017}
}

@inproceedings{ramakrishnan2017empirical,
  title={An empirical evaluation of visual question answering for novel objects},
  author={Ramakrishnan, Santhosh K and Pal, Ambar and Sharma, Gaurav and Mittal, Anurag},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4392--4401},
  year={2017}
}

@article{zhang2019empirical,
  title={An empirical study on leveraging scene graphs for visual question answering},
  author={Zhang, Cheng and Chao, Wei-Lun and Xuan, Dong},
  journal={arXiv preprint arXiv:1907.12133},
  year={2019}
}

@article{agrawal2016analyzing,
  title={Analyzing the behavior of visual question answering models},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1606.07356},
  year={2016}
}

@inproceedings{shrestha2019answer,
  title={Answer them all! toward universal visual question answering models},
  author={Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={10472--10481},
  year={2019}
}

@inproceedings{kafle2016answer,
  title={Answer-type prediction for visual question answering},
  author={Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4976--4984},
  year={2016}
}

@inproceedings{van2019disentangled,
  title={Are Disentangled Representations Helpful for Abstract Visual Reasoning?},
  author={van Steenkiste, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14245--14258},
  year={2019}
}

@inproceedings{kembhavi2017you,
  title={Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
  author={Kembhavi, Aniruddha and Seo, Minjoon and Schwenk, Dustin and Choi, Jonghyun and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4999--5007},
  year={2017}
}

@inproceedings{wu2016ask,
  title={Ask me anything: Free-form visual question answering based on knowledge from external sources},
  author={Wu, Qi and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4622--4630},
  year={2016}
}

@inproceedings{xu2016ask,
  title={Ask, attend and answer: Exploring question-guided spatial attention for visual question answering},
  author={Xu, Huijuan and Saenko, Kate},
  booktitle={European Conference on Computer Vision},
  pages={451--466},
  year={2016},
  organization={Springer}
}

@article{zhang2017asking,
  title={Asking the difficult questions: Goal-oriented visual question generation via intermediate rewards},
  author={Zhang, Junjie and Wu, Qi and Shen, Chunhua and Zhang, Jian and Lu, Jianfeng and Hengel, Anton van den},
  journal={arXiv preprint arXiv:1711.07614},
  year={2017}
}

@article{huang2019assessing,
  title={Assessing the Robustness of Visual Question Answering},
  author={Huang, Jia-Hong and Alfadly, Modar and Ghanem, Bernard and Worring, Marcel},
  journal={arXiv preprint arXiv:1912.01452},
  year={2019}
}

@article{singh2018attention,
  title={Attention on attention: Architectures for visual question answering (vqa)},
  author={Singh, Jasdeep and Ying, Vincent and Nutkiewicz, Alex},
  journal={arXiv preprint arXiv:1803.07724},
  year={2018}
}

@article{chao2017being,
  title={Being negative but constructively: Lessons learnt from creating better visual question answering datasets},
  author={Chao, Wei-Lun and Hu, Hexiang and Sha, Fei},
  journal={arXiv preprint arXiv:1704.07121},
  year={2017}
}

@article{yu2018beyond,
  title={Beyond bilinear: Generalized multimodal factorized high-order pooling for visual question answering},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Fan, Jianping and Tao, Dacheng},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={12},
  pages={5947--5959},
  year={2018},
  publisher={IEEE}
}

@inproceedings{kim2018bilinear,
  title={Bilinear attention networks},
  author={Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1564--1574},
  year={2018}
}

@article{guo2019bilinear,
  title={Bilinear Graph Networks for Visual Question Answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv},
  pages={arXiv--1907},
  year={2019}
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6077--6086},
  year={2018}
}

@article{fang2019btdp,
  title={BTDP: Toward Sparse Fusion with Block Term Decomposition Pooling for Visual Question Answering},
  author={Fang, Zhiwei and Liu, Jing and Liu, Xueliang and Tang, Qu and Li, Yong and Lu, Hanqing},
  journal={ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  volume={15},
  number={2s},
  pages={1--21},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{zhu2015building,
  title={Building a large-scale multimodal knowledge base system for answering visual queries},
  author={Zhu, Yuke and Zhang, Ce and R{\'e}, Christopher and Fei-Fei, Li},
  journal={arXiv preprint arXiv:1507.05670},
  year={2015}
}

@article{guo2019bilinear,
  title={Bilinear Graph Networks for Visual Question Answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv},
  pages={arXiv--1907},
  year={2019}
}

@article{agrawal2017c,
  title={C-vqa: A compositional split of the visual question answering (vqa) v1. 0 dataset},
  author={Agrawal, Aishwarya and Kembhavi, Aniruddha and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1704.08243},
  year={2017}
}

@article{ren2020cgmvqa,
  title={CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering},
  author={Ren, Fuji and Zhou, Yangyang},
  journal={IEEE Access},
  volume={8},
  pages={50626--50636},
  year={2020},
  publisher={IEEE}
}

@inproceedings{wu2018chain,
  title={Chain of reasoning for visual question answering},
  author={Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={275--285},
  year={2018}
}

@inproceedings{sharma2019chartnet,
  title={ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks},
  author={Sharma, Monika and Gupta, Shikha and Chowdhury, Arindam and Vig, Lovekesh},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2901--2910},
  year={2017}
}

@article{yang2019co,
  title={Co-attention network with question type for visual question answering},
  author={Yang, Chao and Jiang, Mengqi and Jiang, Bin and Zhou, Weixin and Li, Keqin},
  journal={IEEE Access},
  volume={7},
  pages={40771--40781},
  year={2019},
  publisher={IEEE}
}

@article{dragomir2018combining,
  title={Combining Visual and Textual Attention in Neural Models for Enhanced Visual Question Answering},
  author={Dragomir, Cosmin and Ojog, Cristian and Rebedea, Traian},
  journal={International Journal of User-System Interaction},
  volume={11},
  number={1},
  pages={1--27},
  year={2018},
  publisher={Matrix Rom}
}

@article{kolling2020component,
  title={Component Analysis for Visual Question Answering Architectures},
  author={Kolling, Camila and Wehrmann, J{\^o}natas and Barros, Rodrigo C},
  journal={arXiv preprint arXiv:2002.05104},
  year={2020}
}

@article{hudson2018compositional,
  title={Compositional attention networks for machine reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={arXiv preprint arXiv:1803.03067},
  year={2018}
}

@article{jiang2015compositional,
  title={Compositional memory for visual question answering},
  author={Jiang, Aiwen and Wang, Fang and Porikli, Fatih and Li, Yi},
  journal={arXiv preprint arXiv:1511.05676},
  year={2015}
}

@inproceedings{chen2020counterfactual,
  title={Counterfactual samples synthesizing for robust visual question answering},
  author={Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10800--10809},
  year={2020}
}

@inproceedings{abbasnejad2020counterfactual,
  title={Counterfactual vision and language learning},
  author={Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and Hengel, Anton van den},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10044--10054},
  year={2020}
}


@article{zheng2020cross,
  title={Cross-Modality Relevance for Reasoning on Language and Vision},
  author={Zheng, Chen and Guo, Quan and Kordjamshidi, Parisa},
  journal={arXiv preprint arXiv:2005.06035},
  year={2020}
}

@inproceedings{huang2018cs,
  title={CS-VQA: visual question answering with compressively sensed images},
  author={Huang, Li-Chi and Kulkarni, Kuldeep and Jha, Anik and Lohit, Suhas and Jayasuriya, Suren and Turaga, Pavan},
  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},
  pages={1283--1287},
  year={2018},
  organization={IEEE}
}

@inproceedings{kafle2017data,
  title={Data augmentation for visual question answering},
  author={Kafle, Kushal and Yousefhussien, Mohammed and Kanan, Christopher},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={198--202},
  year={2017}
}

@inproceedings{bai2018deep,
  title={Deep attention neural tensor network for visual question answering},
  author={Bai, Yalong and Fu, Jianlong and Zhao, Tiejun and Mei, Tao},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}

@inproceedings{yu2019deep,
  title={Deep modular co-attention networks for visual question answering},
  author={Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6281--6290},
  year={2019}
}

@article{yu2020deep,
  title={Deep Multimodal Neural Architecture Search},
  author={Yu, Zhou and Cui, Yuhao and Yu, Jun and Wang, Meng and Tao, Dacheng and Tian, Qi},
  journal={arXiv preprint arXiv:2004.12070},
  year={2020}
}

@article{wu2019deep,
  title={Deep Reason: A Strong Baseline for Real-World Visual Reasoning},
  author={Wu, Chenfei and Zhou, Yanzhao and Li, Gen and Duan, Nan and Tang, Duyu and Wang, Xiaojie},
  journal={arXiv preprint arXiv:1905.10226},
  year={2019}
}

@inproceedings{liu2019densely,
  title={Densely Connected Attention Flow for Visual Question Answering.},
  author={Liu, Fei and Liu, Jing and Fang, Zhiwei and Hong, Richang and Lu, Hanqing},
  booktitle={IJCAI},
  pages={869--875},
  year={2019}
}

@article{sampat2020diverse,
  title={Diverse Visuo-Lingustic Question Answering (DVLQA) Challenge},
  author={Sampat, Shailaja and Yang, Yezhou and Baral, Chitta},
  journal={arXiv preprint arXiv:2005.00330},
  year={2020}
}

@article{mathew2020docvqa,
  title={DocVQA: A Dataset for VQA on Document Images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  journal={arXiv preprint arXiv:2007.00398},
  year={2020}
}

@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4971--4980},
  year={2018}
}

@article{osman2019drau,
  title={DRAU: dual recurrent attention units for visual question answering},
  author={Osman, Ahmed and Samek, Wojciech},
  journal={Computer Vision and Image Understanding},
  volume={185},
  pages={24--30},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{patro2018differential,
  title={Differential attention for visual question answering},
  author={Patro, Badri and Namboodiri, Vinay P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7680--7688},
  year={2018}
}

@inproceedings{nam2017dual,
  title={Dual attention networks for multimodal reasoning and matching},
  author={Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={299--307},
  year={2017}
}

@inproceedings{saito2017dualnet,
  title={Dualnet: Domain-invariant network for visual question answering},
  author={Saito, Kuniaki and Shin, Andrew and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={2017 IEEE International Conference on Multimedia and Expo (ICME)},
  pages={829--834},
  year={2017},
  organization={IEEE}
}

@inproceedings{kafle2018dvqa,
  title={DVQA: Understanding data visualizations via question answering},
  author={Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5648--5656},
  year={2018}
}

@inproceedings{xiong2016dynamic,
  title={Dynamic memory networks for visual and textual question answering},
  author={Xiong, Caiming and Merity, Stephen and Socher, Richard},
  booktitle={International conference on machine learning},
  pages={2397--2406},
  year={2016}
}

@article{chaplot2019embodied,
  title={Embodied Multimodal Multitask Learning},
  author={Chaplot, Devendra Singh and Lee, Lisa and Salakhutdinov, Ruslan and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1902.01385},
  year={2019}
}

@inproceedings{fang2018enhancing,
  title={Enhancing visual question answering using dropout},
  author={Fang, Zhiwei and Liu, Jing and Qiao, Yanyuan and Tang, Qu and Li, Yong and Lu, Hanqing},
  booktitle={Proceedings of the 26th ACM international conference on Multimedia},
  pages={1002--1010},
  year={2018}
}

@article{yu2020ernie,
  title={ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph},
  author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2006.16934},
  year={2020}
}

@article{cao2019explainable,
  title={Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network},
  author={Cao, Qingxing and Li, Bailin and Liang, Xiaodan and Lin, Liang},
  journal={arXiv preprint arXiv:1909.10128},
  year={2019}
}

@inproceedings{manjunatha2019explicit,
  title={Explicit bias discovery in visual question answering models},
  author={Manjunatha, Varun and Saini, Nirat and Davis, Larry S},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={9562--9571},
  year={2019}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@inproceedings{aditya2018explicit,
  title={Explicit reasoning over end-to-end neural architectures for visual question answering},
  author={Aditya, Somak and Yang, Yezhou and Baral, Chitta},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{qiao2018exploring,
  title={Exploring human-like attention supervision in visual question answering},
  author={Qiao, Tingting and Dong, Jianfeng and Xu, Duanqing},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{ren2015exploring,
  title={Exploring models and data for image question answering},
  author={Ren, Mengye and Kiros, Ryan and Zemel, Richard},
  booktitle={Advances in neural information processing systems},
  pages={2953--2961},
  year={2015}
}

@article{halbe2020exploring,
  title={Exploring Weaknesses of VQA Models through Attribution Driven Insights},
  author={Halbe, Shaunak},
  journal={arXiv preprint arXiv:2006.06637},
  year={2020}
}

@inproceedings{dong2018fast,
  title={Fast parameter adaptation for few-shot image captioning and visual question answering},
  author={Dong, Xuanyi and Zhu, Linchao and Zhang, De and Yang, Yi and Wu, Fei},
  booktitle={Proceedings of the 26th ACM international conference on Multimedia},
  pages={54--62},
  year={2018}
}

@inproceedings{lin2018feature,
  title={Feature Enhancement in Attention for Visual Question Answering.},
  author={Lin, Yuetan and Pang, Zhangyang and Wang, Donghui and Zhuang, Yueting},
  booktitle={IJCAI},
  pages={4216--4222},
  year={2018}
}

@article{ebrahimi2017figureqa,
  title={FigureQA: An Annotated Figure Dataset for Visual Reasoning},
  author={Ebrahimi Kahou, Samira and Michalski, Vincent and Atkinson, Adam and Kadar, Akos and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv},
  pages={arXiv--1710},
  year={2017}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{liang2018focal,
  title={Focal visual-text attention for visual question answering},
  author={Liang, Junwei and Jiang, Lu and Cao, Liangliang and Li, Li-Jia and Hauptmann, Alexander G},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6135--6143},
  year={2018}
}

@article{farazi2018known,
  title={From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts},
  author={Farazi, Moshiur R and Khan, Salman H and Barnes, Nick},
  journal={arXiv preprint arXiv:1811.12772},
  year={2018}
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6720--6731},
  year={2019}
}

@article{gao2019two,
  title={From two graphs to n questions: A vqa dataset for compositional reasoning on vision and commonsense},
  author={Gao, Difei and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  journal={arXiv preprint arXiv:1908.02962},
  year={2019}
}

@article{wang2018fvqa,
  title={Fvqa: Fact-based visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={10},
  pages={2413--2427},
  year={2018},
  publisher={IEEE}
}

@article{lu2019good,
  title={Good, Better, Best: Textual Distractors Generation for Multi-Choice VQA via Policy Gradient},
  author={Lu, Jiaying and Ye, Xin and Ren, Yi and Yang, Yezhou},
  journal={arXiv preprint arXiv:1910.09134},
  year={2019}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@article{guo2019graph,
  title={Graph reasoning networks for visual question answering},
  author={Guo, Dalu and Xu, Chang and Tao, Dacheng},
  journal={arXiv preprint arXiv:1907.09815},
  year={2019}
}

@inproceedings{teney2017graph,
  title={Graph-structured representations for visual question answering},
  author={Teney, Damien and Liu, Lingqiao and van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2017}
}

@article{kim2016hadamard,
  title={Hadamard product for low-rank bilinear pooling},
  author={Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:1610.04325},
  year={2016}
}

@article{malinowski2015hard,
  title={Hard to cheat: A turing test based on answering questions about images},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={arXiv preprint arXiv:1501.03302},
  year={2015}
}

@inproceedings{yu2019heterogeneous,
  title={Heterogeneous Graph Learning for Visual Commonsense Reasoning},
  author={Yu, Weijiang and Zhou, Jingwen and Yu, Weihao and Liang, Xiaodan and Xiao, Nong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2769--2779},
  year={2019}
}

@inproceedings{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={Advances in neural information processing systems},
  pages={289--297},
  year={2016}
}

@inproceedings{schwartz2017high,
  title={High-order attention models for visual question answering},
  author={Schwartz, Idan and Schwing, Alexander and Hazan, Tamir},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3664--3674},
  year={2017}
}

@inproceedings{kuhnle2018clever,
  title={How clever is the FiLM model, and how clever can it be},
  author={Kuhnle, Alexander and Xie, Huiyuan and Copestake, Ann},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={0--0},
  year={2018}
}

@article{das2017human,
  title={Human attention in visual question answering: Do humans and deep networks look at the same regions?},
  author={Das, Abhishek and Agrawal, Harsh and Zitnick, Larry and Parikh, Devi and Batra, Dhruv},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={90--100},
  year={2017},
  publisher={Elsevier}
}

@article{wu2017image,
  title={Image captioning and visual question answering based on attributes and external knowledge},
  author={Wu, Qi and Shen, Chunhua and Wang, Peng and Dick, Anthony and van den Hengel, Anton},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={6},
  pages={1367--1381},
  year={2017},
  publisher={IEEE}
}

@inproceedings{noh2016image,
  title={Image question answering using convolutional neural network with dynamic parameter prediction},
  author={Noh, Hyeonwoo and Hongsuck Seo, Paul and Han, Bohyung},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={30--38},
  year={2016}
}

@article{meng2017image,
  title={Image-Question-Linguistic Co-Attention for Visual Question Answering},
  author={Meng, Chenyue and Wang, Yixin and Zhang, Shutong},
  year={2017}
}

@article{qi2020imagebert,
  title={Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data},
  author={Qi, Di and Su, Lin and Song, Jia and Cui, Edward and Bharti, Taroon and Sacheti, Arun},
  journal={arXiv preprint arXiv:2001.07966},
  year={2020}
}

@inproceedings{nguyen2018improved,
  title={Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering},
  author={Nguyen, Duy-Kien and Okatani, Takayuki},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6087--6096},
  year={2018}
}

@article{wu2020improving,
  title={Improving VQA and its Explanations$\backslash$$\backslash$by Comparing Competing Explanations},
  author={Wu, Jialin and Chen, Liyan and Mooney, Raymond J},
  journal={arXiv preprint arXiv:2006.15631},
  year={2020}
}

@inproceedings{jiang2020defense,
  title={In Defense of Grid Features for Visual Question Answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10267--10276},
  year={2020}
}

@article{li2017incorporating,
  title={Incorporating external knowledge to answer open-domain visual questions with dynamic memory networks},
  author={Li, Guohao and Su, Hang and Zhu, Wenwu},
  journal={arXiv preprint arXiv:1712.00733},
  year={2017}
}

@inproceedings{johnson2017inferring,
  title={Inferring and executing programs for visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Hoffman, Judy and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2989--2998},
  year={2017}
}

@article{lin2020interbert,
  title={InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining},
  author={Lin, Junyang and Yang, An and Zhang, Yichang and Liu, Jie and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint arXiv:2003.13198},
  year={2020}
}

@inproceedings{trott2018interpretable,
  title={Interpretable Counting for Visual Question Answering},
  author={Trott, Alexander and Xiong, Caiming and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{zhang2019interpretable,
  title={Interpretable visual question answering by visual grounding from attention supervision mining},
  author={Zhang, Yundong and Niebles, Juan Carlos and Soto, Alvaro},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={349--357},
  year={2019},
  organization={IEEE}
}

@article{goel2020iq,
  title={IQ-VQA: Intelligent Visual Question Answering},
  author={Goel, Vatsal and Chandak, Mohit and Anand, Ashish and Guha, Prithwijit},
  journal={arXiv preprint arXiv:2007.04422},
  year={2020}
}

@inproceedings{gordon2018iqa,
  title={Iqa: Visual question answering in interactive environments},
  author={Gordon, Daniel and Kembhavi, Aniruddha and Rastegari, Mohammad and Redmon, Joseph and Fox, Dieter and Farhadi, Ali},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4089--4098},
  year={2018}
}

@inproceedings{liu2018ivqa,
  title={iVQA: Inverse visual question answering},
  author={Liu, Feng and Xiang, Tao and Hospedales, Timothy M and Yang, Wankou and Sun, Changyin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8611--8619},
  year={2018}
}

@inproceedings{zhu2017knowledge,
  title={Knowledge acquisition for visual question answering via iterative querying},
  author={Zhu, Yuke and Lim, Joseph J and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1154--1163},
  year={2017}
}

@inproceedings{shah2019kvqa,
  title={Kvqa: Knowledge-aware visual question answering},
  author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8876--8884},
  year={2019}
}

@article{gan2020large,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:2006.06195},
  year={2020}
}

@inproceedings{hu2018learning,
  title={Learning answer embeddings for visual question answering},
  author={Hu, Hexiang and Chao, Wei-Lun and Sha, Fei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5428--5436},
  year={2018}
}

@inproceedings{hudson2019learning,
  title={Learning by abstraction: The neural state machine},
  author={Hudson, Drew and Manning, Christopher D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5903--5916},
  year={2019}
}

@inproceedings{norcliffe2018learning,
  title={Learning conditioned graph structures for interpretable visual question answering},
  author={Norcliffe-Brown, Will and Vafeias, Stathis and Parisot, Sarah},
  booktitle={Advances in neural information processing systems},
  pages={8334--8343},
  year={2018}
}

@article{liu2019learning,
  title={Learning Rich Image Region Representation for Visual Question Answering},
  author={Liu, Bei and Huang, Zhicheng and Zeng, Zhaoyang and Chen, Zheyu and Fu, Jianlong},
  journal={arXiv preprint arXiv:1910.13077},
  year={2019}
}

@article{pahuja2019learning,
  title={Learning Sparse Mixture of Experts for Visual Question Answering},
  author={Pahuja, Vardaan and Fu, Jie and Pal, Christopher J},
  journal={arXiv preprint arXiv:1909.09192},
  year={2019}
}

@inproceedings{ma2016learning,
  title={Learning to answer questions from image using convolutional neural network},
  author={Ma, Lin and Lu, Zhengdong and Li, Hang},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}

@article{andreas2016learning,
  title={Learning to compose neural networks for question answering},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  journal={arXiv preprint arXiv:1601.01705},
  year={2016}
}

@inproceedings{hu2017learning,
  title={Learning to reason: End-to-end module networks for visual question answering},
  author={Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={804--813},
  year={2017}
}

@article{zhang2018learning,
  title={Learning to count objects in natural images for visual question answering},
  author={Zhang, Yan and Hare, Jonathon and Pr{\"u}gel-Bennett, Adam},
  journal={arXiv preprint arXiv:1802.05766},
  year={2018}
}

@inproceedings{su2018learning,
  title={Learning visual knowledge memory networks for visual question answering},
  author={Su, Zhou and Zhu, Chen and Dong, Yinpeng and Cai, Dongqi and Chen, Yurong and Li, Jianguo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7736--7745},
  year={2018}
}

@article{cao2020linguistically,
  title={Linguistically Driven Graph Capsule Network for Visual Question Reasoning},
  author={Cao, Qingxing and Liang, Xiaodan and Wang, Keze and Lin, Liang},
  journal={arXiv preprint arXiv:2003.10065},
  year={2020}
}

@article{huang2020m3p,
  title={M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training},
  author={Huang, Haoyang and Su, Lin and Qi, Di and Duan, Nan and Cui, Edward and Bharti, Taroon and Zhang, Lei and Wang, Lijuan and Gao, Jianfeng and Liu, Bei and others},
  journal={arXiv preprint arXiv:2006.02635},
  year={2020}
}

@inproceedings{goyal2017making,
  title={Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6904--6913},
  year={2017}
}

@article{zitnick2016measuring,
  title={Measuring machine intelligence through visual question answering},
  author={Zitnick, C Lawrence and Agrawal, Aishwarya and Antol, Stanislaw and Mitchell, Margaret and Batra, Dhruv and Parikh, Devi},
  journal={AI Magazine},
  volume={37},
  number={1},
  pages={63--72},
  year={2016}
}

@article{zhu2020mucko,
  title={Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based VisualQuestion Answering},
  author={Zhu, Zihao and Yu, Jing and Wang, Yujing and Sun, Yajing and Hu, Yue and Wu, Qi},
  journal={arXiv preprint arXiv:2006.09073},
  year={2020}
}

@inproceedings{yu2017multi,
  title={Multi-level attention networks for visual question answering},
  author={Yu, Dongfei and Fu, Jianlong and Mei, Tao and Rui, Yong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4709--4717},
  year={2017}
}

@inproceedings{yu2017multi,
  title={Multi-modal factorized bilinear pooling with co-attention learning for visual question answering},
  author={Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1821--1830},
  year={2017}
}

@article{fukui2016multimodal,
  title={Multimodal compact bilinear pooling for visual question answering and visual grounding},
  author={Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1606.01847},
  year={2016}
}

@article{patro2018multimodal,
  title={Multimodal differential network for visual question generation},
  author={Patro, Badri N and Kumar, Sandeep and Kurmi, Vinod K and Namboodiri, Vinay P},
  journal={arXiv preprint arXiv:1808.03986},
  year={2018}
}

@article{park2016attentive,
  title={Attentive explanations: Justifying decisions and pointing to the evidence},
  author={Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1612.04757},
  year={2016}
}

@article{gomez2020multimodal,
  title={Multimodal grid features and cell pointers for Scene Text Visual Question Answering},
  author={G{\'o}mez, Llu{\'\i}s and Biten, Ali Furkan and Tito, Rub{\`e}n and Mafla, Andr{\'e}s and Karatzas, Dimosthenis},
  journal={arXiv preprint arXiv:2006.00923},
  year={2020}
}

@inproceedings{ilievski2017multimodal,
  title={Multimodal learning and reasoning for visual question answering},
  author={Ilievski, Ilija and Feng, Jiashi},
  booktitle={Advances in neural information processing systems},
  pages={551--562},
  year={2017}
}

@inproceedings{khademi2020multimodal,
  title={Multimodal Neural Graph Memory Networks for Visual Question Answering},
  author={Khademi, Mahmoud},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7177--7188},
  year={2020}
}

@inproceedings{kim2016multimodal,
  title={Multimodal residual learning for visual qa},
  author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  booktitle={Advances in neural information processing systems},
  pages={361--369},
  year={2016}
}

@inproceedings{cadene2019murel,
  title={Murel: Multimodal relational reasoning for visual question answering},
  author={Cadene, Remi and Ben-Younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1989--1998},
  year={2019}
}

@inproceedings{ben2017mutan,
  title={Mutan: Multimodal tucker fusion for visual question answering},
  author={Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2612--2620},
  year={2017}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={39--48},
  year={2016}
}

@inproceedings{yi2018neural,
  title={Neural-symbolic vqa: Disentangling reasoning from vision and language understanding},
  author={Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Kohli, Pushmeet and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={1031--1042},
  year={2018}
}

@article{amizadeh2020neuro,
  title={Neuro-Symbolic Visual Reasoning: Disentangling" Visual" from" Reasoning"},
  author={Amizadeh, Saeed and Palangi, Hamid and Polozov, Oleksandr and Huang, Yichen and Koishida, Kazuhito},
  journal={arXiv preprint arXiv:2006.11524},
  year={2020}
}

@inproceedings{desta2018object,
  title={Object-based reasoning in VQA},
  author={Desta, Mikyas T and Chen, Larry and Kornuta, Tomasz},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1814--1823},
  year={2018},
  organization={IEEE}
}

@article{zhu2020object,
  title={Object-difference drived graph convolutional networks for visual question answering},
  author={Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
  journal={Multimedia Tools and Applications},
  pages={1--19},
  year={2020},
  publisher={Springer}
}

@inproceedings{marino2019ok,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3195--3204},
  year={2019}
}

@inproceedings{wang2020general,
  title={On the general value of evidence, and bilingual scene-text visual question answering},
  author={Wang, Xinyu and Liu, Yuliang and Shen, Chunhua and Ng, Chun Chet and Luo, Canjie and Jin, Lianwen and Chan, Chee Seng and Hengel, Anton van den and Wang, Liangwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10126--10135},
  year={2020}
}

@article{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Hu, Xiaowei and Zhang, Pengchuan and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  journal={arXiv preprint arXiv:2004.06165},
  year={2020}
}

@inproceedings{narasimhan2018out,
  title={Out of the box: Reasoning with graph convolution nets for factual visual question answering},
  author={Narasimhan, Medhini and Lazebnik, Svetlana and Schwing, Alexander},
  booktitle={Advances in neural information processing systems},
  pages={2654--2665},
  year={2018}
}

@article{dancette2020overcoming,
  title={Overcoming Statistical Shortcuts for Open-ended Visual Counting},
  author={Dancette, Corentin and Cadene, Remi and Chen, Xinlei and Cord, Matthieu},
  journal={arXiv preprint arXiv:2006.10079},
  year={2020}
}

@inproceedings{jing2020overcoming,
  title={Overcoming Language Priors in VQA via Decomposed Linguistic Representations.},
  author={Jing, Chenchen and Wu, Yuwei and Zhang, Xiaoxun and Jia, Yunde and Wu, Qi},
  booktitle={AAAI},
  pages={11181--11188},
  year={2020}
}


@article{jolly2020p,
  title={P $$\backslash$approx $ NP, at least in Visual Question Answering},
  author={Jolly, Shailza and Palacio, Sebastian and Folz, Joachim and Raue, Federico and Hees, Jorn and Dengel, Andreas},
  journal={arXiv preprint arXiv:2003.11844},
  year={2020}
}

@article{he2020pathvqa,
  title={PathVQA: 30000+ Questions for Medical Visual Question Answering},
  author={He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal={arXiv preprint arXiv:2003.10286},
  year={2020}
}

@article{vedantam2019probabilistic,
  title={Probabilistic neural-symbolic models for interpretable visual question answering},
  author={Vedantam, Ramakrishna and Desai, Karan and Lee, Stefan and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1902.07864},
  year={2019}
}

@article{greco2019psycholinguistics,
  title={Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering},
  author={Greco, Claudio and Plank, Barbara and Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  journal={arXiv preprint arXiv:1906.04229},
  year={2019}
}

@article{jiang2018pythia,
  title={Pythia v0. 1: the winning entry to the vqa challenge 2018},
  author={Jiang, Yu and Natarajan, Vivek and Chen, Xinlei and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1807.09956},
  year={2018}
}

@article{prabhakar2018question,
  title={Question Relevance in Visual Question Answering},
  author={Prabhakar, Prakruthi and Kulkarni, Nitish and Zhang, Linghao},
  journal={arXiv preprint arXiv:1807.08435},
  year={2018}
}

@article{ray2016question,
  title={Question relevance in VQA: identifying non-visual and false-premise questions},
  author={Ray, Arijit and Christie, Gordon and Bansal, Mohit and Batra, Dhruv and Parikh, Devi},
  journal={arXiv preprint arXiv:1606.06622},
  year={2016}
}

@inproceedings{gao2018question,
  title={Question-guided hybrid convolution for visual question answering},
  author={Gao, Peng and Li, Hongsheng and Li, Shuang and Lu, Pan and Li, Yikang and Hoi, Steven CH and Wang, Xiaogang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={469--485},
  year={2018}
}

@inproceedings{lu2018r,
  title={R-VQA: learning visual relation facts with semantic attention for visual question answering},
  author={Lu, Pan and Ji, Lei and Zhang, Wei and Duan, Nan and Zhou, Ming and Wang, Jianyong},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1880--1889},
  year={2018}
}

@article{yagcioglu2018recipeqa,
  title={Recipeqa: A challenge dataset for multimodal comprehension of cooking recipes},
  author={Yagcioglu, Semih and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli},
  journal={arXiv preprint arXiv:1809.00812},
  year={2018}
}

@article{farazi2018reciprocal,
  title={Reciprocal attention fusion for visual question answering},
  author={Farazi, Moshiur R and Khan, Salman H},
  journal={arXiv preprint arXiv:1805.04247},
  year={2018}
}

@article{kv2020reducing,
  title={Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder},
  author={KV, Gouthaman and Mittal, Anurag},
  journal={arXiv preprint arXiv:2007.06198},
  year={2020}
}

@inproceedings{li2019relation,
  title={Relation-aware graph attention network for visual question answering},
  author={Li, Linjie and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={10313--10322},
  year={2019}
}

@inproceedings{jabri2016revisiting,
  title={Revisiting visual question answering baselines},
  author={Jabri, Allan and Joulin, Armand and Van Der Maaten, Laurens},
  booktitle={European conference on computer vision},
  pages={727--739},
  year={2016},
  organization={Springer}
}

@article{huang2017robustness,
  title={Robustness analysis of visual qa models by basic questions},
  author={Huang, Jia-Hong and Dao, Cuong Duc and Alfadly, Modar and Yang, C Huck and Ghanem, Bernard},
  journal={arXiv preprint arXiv:1709.04625},
  year={2017}
}

@article{lobry2020rsvqa,
  title={RSVQA: Visual Question Answering for Remote Sensing Data},
  author={Lobry, Sylvain and Marcos, Diego and Murray, Jesse and Tuia, Devis},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{cadene2019rubi,
  title={Rubi: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  booktitle={Advances in neural information processing systems},
  pages={841--852},
  year={2019}
}

@article{hildebrandt2020scene,
  title={Scene Graph Reasoning for Visual Question Answering},
  author={Hildebrandt, Marcel and Li, Hang and Koner, Rajat and Tresp, Volker and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2007.01072},
  year={2020}
}

@inproceedings{wu2019self,
  title={Self-critical reasoning for robust visual question answering},
  author={Wu, Jialin and Mooney, Raymond},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8604--8614},
  year={2019}
}

@article{sur2020self,
  title={Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering},
  author={Sur, Chiranjib},
  journal={arXiv preprint arXiv:2006.14264},
  year={2020}
}

@article{kazemi2017show,
  title={Show, ask, attend, and answer: A strong baseline for visual question answering},
  author={Kazemi, Vahid and Elqursh, Ali},
  journal={arXiv preprint arXiv:1704.03162},
  year={2017}
}

@article{zhou2015simple,
  title={Simple baseline for visual question answering},
  author={Zhou, Bolei and Tian, Yuandong and Sukhbaatar, Sainbayar and Szlam, Arthur and Fergus, Rob},
  journal={arXiv preprint arXiv:1512.02167},
  year={2015}
}

@inproceedings{selvaraju2020squinting,
  title={SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions},
  author={Selvaraju, Ramprasaath R and Tendulkar, Purva and Parikh, Devi and Horvitz, Eric and Ribeiro, Marco Tulio and Nushi, Besmira and Kamar, Ece},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10003--10011},
  year={2020}
}

@inproceedings{yang2016stacked,
  title={Stacked attention networks for image question answering},
  author={Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={21--29},
  year={2016}
}

@inproceedings{narasimhan2018straight,
  title={Straight to the facts: Learning knowledge base retrieval for factual visual question answering},
  author={Narasimhan, Medhini and Schwing, Alexander G},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={451--468},
  year={2018}
}

@inproceedings{zhu2017structured,
  title={Structured attentions for visual question answering},
  author={Zhu, Chen and Zhao, Yanpeng and Huang, Shuaiyi and Tu, Kewei and Ma, Yi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1291--1300},
  year={2017}
}

@article{gao2020structured,
  title={Structured Multimodal Attentions for TextVQA},
  author={Gao, Chenyu and Zhu, Qi and Wang, Peng and Li, Hui and Liu, Yuliang and Hengel, Anton van den and Wu, Qi},
  journal={arXiv preprint arXiv:2006.00753},
  year={2020}
}

@inproceedings{wang2018structured,
  title={Structured triplet learning with pos-tag guided attention for visual question answering},
  author={Wang, Zhe and Liu, Xiaoyi and Wang, Limin and Qiao, Yu and Xie, Xiaohui and Fowlkes, Charless},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1888--1896},
  year={2018},
  organization={IEEE}
}

@article{gupta2017survey,
  title={Survey of visual question answering: Datasets and techniques},
  author={Gupta, Akshay Kumar},
  journal={arXiv preprint arXiv:1705.03865},
  year={2017}
}

@inproceedings{xiong2020ta,
  title={TA-Student VQA: Multi-Agents Training by Self-Questioning},
  author={Xiong, Peixi and Wu, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10065--10075},
  year={2020}
}

@inproceedings{acharya2019tallyqa,
  title={TallyQA: Answering complex counting questions},
  author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8076--8084},
  year={2019}
}

@inproceedings{selvaraju2019taking,
  title={Taking a hint: Leveraging explanations to make vision and language models more grounded},
  author={Selvaraju, Ramprasaath R and Lee, Stefan and Shen, Yilin and Jin, Hongxia and Ghosh, Shalini and Heck, Larry and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2591--2600},
  year={2019}
}

@article{alipour2020impact,
  title={The Impact of Explanations on AI Competency Prediction in VQA},
  author={Alipour, Kamran and Ray, Arijit and Lin, Xiao and Schulze, Jurgen P and Yao, Yi and Burachas, Giedrius T},
  journal={arXiv preprint arXiv:2007.00900},
  year={2020}
}

@article{malinowski2018visual,
  title={The visual QA devil in the details: The impact of early fusion and batch norm on clevr},
  author={Malinowski, Mateusz and Doersch, Carl},
  journal={arXiv preprint arXiv:1809.04482},
  year={2018}
}

@inproceedings{wang2017vqa,
  title={The vqa-machine: Learning how to use existing vision algorithms to answer new questions},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and van den Hengel, Anton},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1173--1182},
  year={2017}
}

@article{goyal2018think,
  title={Think Visually: Question Answering through Virtual Imagery},
  author={Goyal, Ankit and Wang, Jian and Deng, Jia},
  journal={arXiv preprint arXiv:1805.11025},
  year={2018}
}

@inproceedings{teney2018tips,
  title={Tips and tricks for visual question answering: Learnings from the 2017 challenge},
  author={Teney, Damien and Anderson, Peter and He, Xiaodong and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4223--4232},
  year={2018}
}

@article{malinowski2014towards,
  title={Towards a visual turing challenge},
  author={Malinowski, Mateusz and Fritz, Mario},
  journal={arXiv preprint arXiv:1410.8027},
  year={2014}
}

@inproceedings{agarwal2020towards,
  title={Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing},
  author={Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9690--9698},
  year={2020}
}

@article{goyal2016towards,
  title={Towards transparent ai systems: Interpreting visual question answering models},
  author={Goyal, Yash and Mohapatra, Akrit and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1608.08974},
  year={2016}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8317--8326},
  year={2019}
}

@article{noh2016training,
  title={Training recurrent answering units with joint loss minimization for vqa},
  author={Noh, Hyeonwoo and Han, Bohyung},
  journal={arXiv preprint arXiv:1606.03647},
  year={2016}
}

@inproceedings{mascharka2018transparency,
  title={Transparency by design: Closing the gap between performance and interpretability in visual reasoning},
  author={Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4942--4950},
  year={2018}
}

@article{mogadala2019trends,
  title={Trends in integration of vision and language research: A survey of tasks, datasets, and methods},
  author={Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
  journal={arXiv preprint arXiv:1907.09358},
  year={2019}
}

@article{vatashsky2018understand,
  title={Understand, Compose and Respond-Answering Visual Questions by a Composition of Abstract Procedures},
  author={Vatashsky, Ben Zion and Ullman, Shimon},
  journal={arXiv preprint arXiv:1810.10656},
  year={2018}
}

@inproceedings{zhou2020unified,
  title={Unified Vision-Language Pre-Training for Image Captioning and VQA.},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason J and Gao, Jianfeng},
  booktitle={AAAI},
  pages={13041--13049},
  year={2020}
}

@article{chen2019uniter,
  title={Uniter: Learning universal image-text representations},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:1909.11740},
  year={2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}

@inproceedings{wang2020visual,
  title={Visual commonsense r-cnn},
  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10760--10770},
  year={2020}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  number={1},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@article{yu2015visual,
  title={Visual madlibs: Fill in the blank image generation and question answering},
  author={Yu, Licheng and Park, Eunbyung and Berg, Alexander C and Berg, Tamara L},
  journal={arXiv preprint arXiv:1506.00278},
  year={2015}
}

@article{pollard2020visual,
  title={Visual Question Answering as a Multi-Task Problem},
  author={Pollard, Amelia Elizabeth and Shapiro, Jonathan L},
  journal={arXiv preprint arXiv:2007.01780},
  year={2020}
}

@inproceedings{li2019visual,
  title={Visual question answering as reading comprehension},
  author={Li, Hui and Wang, Peng and Shen, Chunhua and Hengel, Anton van den},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6319--6328},
  year={2019}
}

@article{bongini2020visual,
  title={Visual Question Answering for Cultural Heritage},
  author={Bongini, Pietro and Becattini, Federico and Bagdanov, Andrew D and Del Bimbo, Alberto},
  journal={arXiv preprint arXiv:2003.09853},
  year={2020}
}

@inproceedings{chou2020visual,
  title={Visual Question Answering on 360deg Images},
  author={Chou, Shih-Han and Chao, Wei-Lun and Lai, Wei-Sheng and Sun, Min and Yang, Ming-Hsuan},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={1607--1616},
  year={2020}
}

@inproceedings{lioutas2018visual,
  title={Visual Question Answering using Explicit Visual Attention},
  author={Lioutas, Vasileios and Passalis, Nikolaos and Tefas, Anastasios},
  booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)},
  pages={1--5},
  year={2018},
  organization={IEEE}
}

@inproceedings{ma2018visual,
  title={Visual question answering with memory-augmented networks},
  author={Ma, Chao and Shen, Chunhua and Dick, Anthony and Wu, Qi and Wang, Peng and van den Hengel, Anton and Reid, Ian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6975--6984},
  year={2018}
}

@article{shevchenko2020visual,
  title={Visual Question Answering with Prior Class Semantics},
  author={Shevchenko, Violetta and Teney, Damien and Dick, Anthony and Hengel, Anton van den},
  journal={arXiv preprint arXiv:2005.01239},
  year={2020}
}

@inproceedings{li2016visual,
  title={Visual question answering with question representation update (qru)},
  author={Li, Ruiyu and Jia, Jiaya},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4655--4663},
  year={2016}
}

@article{wu2017visual,
  title={Visual question answering: A survey of methods and datasets},
  author={Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={21--40},
  year={2017},
  publisher={Elsevier}
}

@article{kafle2017visual,
  title={Visual question answering: Datasets, algorithms, and future challenges},
  author={Kafle, Kushal and Kanan, Christopher},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={3--20},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{cao2018visual,
  title={Visual question reasoning on general dependency tree},
  author={Cao, Qingxing and Liang, Xiaodan and Li, Bailing and Li, Guanbin and Lin, Liang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7249--7257},
  year={2018}
}

@article{kim2018visual,
  title={Visual reasoning by progressive module networks},
  author={Kim, Seung Wook and Tapaswi, Makarand and Fidler, Sanja},
  journal={arXiv preprint arXiv:1806.02453},
  year={2018}
}

@article{geman2015visual,
  title={Visual turing test for computer vision systems},
  author={Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={12},
  pages={3618--3623},
  year={2015},
  publisher={National Acad Sciences}
}

@inproceedings{zhu2016visual7w,
  title={Visual7w: Grounded question answering in images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4995--5004},
  year={2016}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{bigham2010vizwiz,
  title={VizWiz: nearly real-time answers to visual questions},
  author={Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and others},
  booktitle={Proceedings of the 23nd annual ACM symposium on User interface software and technology},
  pages={333--342},
  year={2010}
}

@article{su2019vl,
  title={Vl-bert: Pre-training of generic visual-linguistic representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  journal={arXiv preprint arXiv:1908.08530},
  year={2019}
}

@inproceedings{vatashsky2020vqa,
  title={VQA with no questions-answers training},
  author={Vatashsky, Ben-Zion and Ullman, Shimon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10376--10386},
  year={2020}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{wu2016value,
  title={What value do explicit high level concepts have in vision to language problems?},
  author={Wu, Qi and Shen, Chunhua and Liu, Lingqiao and Dick, Anthony and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={203--212},
  year={2016}
}

@inproceedings{shih2016look,
  title={Where to look: Focus regions for visual question answering},
  author={Shih, Kevin J and Singh, Saurabh and Hoiem, Derek},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4613--4621},
  year={2016}
}

@article{terao2020visual,
  title={Which visual questions are difficult to answer? Analysis with Entropy of Answer Distributions},
  author={Terao, Kento and Tamaki, Toru and Raytchev, Bisser and Kaneda, Kazufumi and Satoh, Shun'ichi},
  journal={arXiv preprint arXiv:2004.05595},
  year={2020}
}

@inproceedings{zhang2016yin,
  title={Yin and yang: Balancing and answering binary visual questions},
  author={Zhang, Peng and Goyal, Yash and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5014--5022},
  year={2016}
}

@article{li2018zero,
  title={Zero-Shot Transfer VQA Dataset},
  author={Li, Yuanpeng and Yang, Yi and Wang, Jianyu and Xu, Wei},
  journal={arXiv preprint arXiv:1811.00692},
  year={2018}
}

@article{teney2016zero,
  title={Zero-shot visual question answering},
  author={Teney, Damien and Hengel, Anton van den},
  journal={arXiv preprint arXiv:1611.05546},
  year={2016}
}

@article{tang2020semantic,
  title={Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering},
  author={Tang, Ruixue and Ma, Chao and Zhang, Wei Emma and Wu, Qi and Yang, Xiaokang},
  journal={arXiv preprint arXiv:2007.09592},
  year={2020}
}

@article{mathew2020document,
  title={Document Visual Question Answering Challenge 2020},
  author={Mathew, Minesh and Tito, Ruben and Karatzas, Dimosthenis and Manmatha, R and Jawahar, CV},
  journal={arXiv preprint arXiv:2008.08899},
  year={2020}
}

@inproceedings{malinowski2015ask,
  title={Ask your neurons: A neural-based approach to answering questions about images},
  author={Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1--9},
  year={2015}
}

@inproceedings{chattopadhyay2017counting,
  title={Counting everyday objects in everyday scenes},
  author={Chattopadhyay, Prithvijit and Vedantam, Ramakrishna and Selvaraju, Ramprasaath R and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1135--1144},
  year={2017}
}

@inproceedings{yang2018dataset,
  title={A dataset and architecture for visual reasoning with a working memory},
  author={Yang, Guangyu Robert and Ganichev, Igor and Wang, Xiao-Jing and Shlens, Jonathon and Sussillo, David},
  booktitle={European Conference on Computer Vision},
  pages={729--745},
  year={2018},
  organization={Springer}
}

@article{mao2019neuro,
  title={The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision},
  author={Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B and Wu, Jiajun},
  journal={arXiv preprint arXiv:1904.12584},
  year={2019}
}

@inproceedings{gao2019dynamic,
  title={Dynamic fusion with intra-and inter-modality attention flow for visual question answering},
  author={Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven CH and Wang, Xiaogang and Li, Hongsheng},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6639--6648},
  year={2019}
}

@article{le2020dynamic,
  title={Dynamic Language Binding in Relational Visual Reasoning},
  author={Le, Thao Minh and Le, Vuong and Venkatesh, Svetha and Tran, Truyen},
  journal={arXiv preprint arXiv:2004.14603},
  year={2020}
}

@article{shi2020contrastive,
  title={Contrastive Visual-Linguistic Pretraining},
  author={Shi, Lei and Shuang, Kai and Geng, Shijie and Su, Peng and Jiang, Zhengkai and Gao, Peng and Fu, Zuohui and de Melo, Gerard and Su, Sen},
  journal={arXiv preprint arXiv:2007.13135},
  year={2020}
}

@article{kv2020linguistically,
  title={Linguistically-aware Attention for Reducing the Semantic-Gap in Vision-Language Tasks},
  author={KV, Gouthaman and Nambiar, Athira and Srinivas, Kancheti Sai and Mittal, Anurag},
  journal={arXiv preprint arXiv:2008.08012},
  year={2020}
}

@article{bansal2020visual,
  title={Visual Question Answering on Image Sets},
  author={Bansal, Ankan and Zhang, Yuting and Chellappa, Rama},
  journal={arXiv preprint arXiv:2008.11976},
  year={2020}
}

@article{yu2020cross,
  title={Cross-modal knowledge reasoning for knowledge-based visual question answering},
  author={Yu, Jing and Zhu, Zihao and Wang, Yujing and Zhang, Weifeng and Hu, Yue and Tan, Jianlong},
  journal={Pattern Recognition},
  pages={107563},
  year={2020},
  publisher={Elsevier}
}

@article{garcia_dataset_2020,
  title={A {Dataset} and {Baselines} for {Visual} {Question} {Answering} on {Art}},
  author={Garcia, Noa and Ye, Chentao and Liu, Zihua and Hu, Qingtao and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta and Mitamura, Teruko},
  journal={arXiv:2008.12520},
  year={2020},
}


@article{gao_question-led_2020,
	title = {Question-{Led} object attention for visual question answering},
	volume = {391},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219304163},
	doi = {10.1016/j.neucom.2018.11.102},
	journal = {Neurocomputing},
	author = {Gao, Lianli and Cao, Liangfu and Xu, Xing and Shao, Jie and Song, Jingkuan},
	month = may,
	year = {2020},
}

@article{hong_selective_2020,
	title = {Selective residual learning for {Visual} {Question} {Answering}},
	volume = {402},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220304859},
	doi = {10.1016/j.neucom.2020.03.098},
	journal = {Neurocomputing},
	author = {Hong, Jongkwang and Park, Sungho and Byun, Hyeran},
	year = {2020},
}

@article{toor_question_2019,
	title = {Question action relevance and editing for visual question answering},
	volume = {78},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-018-6097-z},
	journal = {Multimedia Tools and Applications},
	author = {Toor, Andeep S. and Wechsler, Harry and Nappi, Michele},
	year = {2019},
}

@article{hong_exploiting_2019,
	title = {Exploiting hierarchical visual features for visual question answering},
	volume = {351},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231219303753},
	journal = {Neurocomputing},
	author = {Hong, Jongkwang and Fu, Jianlong and Uh, Youngjung and Mei, Tao and Byun, Hyeran},
	year = {2019},
}

@article{liu_inverse_2020,
	title = {Inverse {Visual} {Question} {Answering}: {A} {New} {Benchmark} and {VQA} {Diagnosis} {Tool}},
	volume = {42},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Inverse {Visual} {Question} {Answering}},
	url = {https://ieeexplore.ieee.org/document/8528867/},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Feng and Xiang, Tao and Hospedales, Timothy M. and Yang, Wankou and Sun, Changyin},
	year = {2020},
}

@article{zhu_object-difference_2020,
	title = {Object-difference drived graph convolutional networks for visual question answering},
	issn = {1380-7501, 1573-7721},
	url = {http://link.springer.com/10.1007/s11042-020-08790-0},
	journal = {Multimedia Tools and Applications},
	author = {Zhu, Xi and Mao, Zhendong and Chen, Zhineng and Li, Yangyang and Wang, Zhaohui and Wang, Bin},
	year = {2020},
}

@article{zhang_multimodal_2020,
	title = {Multimodal feature fusion by relational reasoning and attention for visual question answering},
	volume = {55},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253518308248},
	journal = {Information Fusion},
	author = {Zhang, Weifeng and Yu, Jing and Hu, Hua and Hu, Haiyang and Qin, Zengchang},
	year = {2020},
}

@article{liu_visual_2020,
	title = {Visual {Question} {Answering} via {Combining} {Inferential} {Attention} and {Semantic} {Space} {Mapping}},
	volume = {207},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120304962},
	journal = {Knowledge-Based Systems},
	author = {Liu, Yun and Zhang, Xiaoming and Huang, Feiran and Zhou, Zhibo and Zhao, Zhonghua and Li, Zhoujun},
	year = {2020},
}

@article{long_repeatpadding_2020,
	title = {{RepeatPadding}: {Balancing} words and sentence length for language comprehension in visual question answering},
	volume = {529},
	issn = {00200255},
	shorttitle = {{RepeatPadding}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002002552030342X},
	journal = {Information Sciences},
	author = {Long, Yu and Tang, Pengjie and Wei, Zhihua and Gu, Jinjing and Wang, Hanli},
	year = {2020},
}

@article{zhang_information_2019,
	title = {Information fusion in visual question answering: {A} {Survey}},
	volume = {52},
	issn = {15662535},
	shorttitle = {Information fusion in visual question answering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253518308893},
	journal = {Information Fusion},
	author = {Zhang, Dongxiang and Cao, Rui and Wu, Sai},
	year = {2019},
}

@article{ruwa_mood-aware_2019,
	title = {Mood-aware visual question answering},
	volume = {330},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231218313808},
	doi = {10.1016/j.neucom.2018.11.049},
	journal = {Neurocomputing},
	author = {Ruwa, Nelson and Mao, Qirong and Wang, Liangjun and Gou, Jianping and Dong, Ming},
	month = feb,
	year = {2019},
}

@article{peng_word--region_2019,
	title = {Word-to-region attention network for visual question answering},
	volume = {78},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-018-6389-3},
	doi = {10.1007/s11042-018-6389-3},
	journal = {Multimedia Tools and Applications},
	author = {Peng, Liang and Yang, Yang and Bin, Yi and Xie, Ning and Shen, Fumin and Ji, Yanli and Xu, Xing},
	year = {2019},
}

@article{cho_x-lxmert_2020,
	title = {X-{LXMERT}: {Paint}, {Caption} and {Answer} {Questions} with {Multi}-{Modal} {Transformers}},
	shorttitle = {X-{LXMERT}},
	url = {http://arxiv.org/abs/2009.11278},
	journal = {arXiv:2009.11278 [cs]},
	author = {Cho, Jaemin and Lu, Jiasen and Schwenk, Dustin and Hajishirzi, Hannaneh and Kembhavi, Aniruddha},
	year = {2020},
}

@article{do_multiple_2020,
	title = {Multiple interaction learning with question-type prior knowledge for constraining answer search space in visual question answering},
	url = {http://arxiv.org/abs/2009.11118},
	journal = {arXiv:2009.11118 [cs]},
	author = {Do, Tuong and Nguyen, Binh X. and Tran, Huy and Tjiputra, Erman and Tran, Quang D. and Do, Thanh-Toan},
	year = {2020},
}

